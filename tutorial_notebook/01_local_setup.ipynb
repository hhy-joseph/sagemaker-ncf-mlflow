{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development Environment Setup for NCF\n",
    "\n",
    "This notebook guides you through setting up your local development environment for Neural Collaborative Filtering (NCF) using SageMaker local mode with GPU support.\n",
    "\n",
    "## Prerequisites\n",
    "- Windows 10/11 with NVIDIA GPU/ Mac with GPU\n",
    "- Python 3.10+\n",
    "- Administrative access for Docker installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check System Requirements\n",
    "\n",
    "First, let's verify that your system meets the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n",
      "Operating System: Windows 11\n",
      "NVIDIA GPU detected:\n",
      "\n",
      "Sun Feb 16 19:10:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8              1W /   45W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import logging\n",
    "\n",
    "# Configure logging to suppress file path warnings\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)\n",
    "logging.getLogger('botocore.credentials').setLevel(logging.ERROR)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "\n",
    "# Check for GPU availability using nvidia-smi\n",
    "import subprocess\n",
    "try:\n",
    "    nvidia_smi = subprocess.check_output([\"nvidia-smi\"])\n",
    "    print(\"NVIDIA GPU detected:\\n\")\n",
    "    print(nvidia_smi.decode())\n",
    "except:\n",
    "    print(\"NVIDIA GPU not detected or nvidia-smi not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Python Packages\n",
    "\n",
    "Install the necessary Python packages for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv for package management\n",
    "# # On Windows.\n",
    "# powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n",
    "# On macOS and Linux.\n",
    "# curl -LsSf https://astral.sh/uv/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: C:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 145ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pip \u001b[2m(1.8MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloaded\u001b[0m\u001b[39m pip\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 426ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 574ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.0.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.9 environment at: C:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 3.66s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 8.34s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==70.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu118\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0+cu118\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu118\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.9 environment at: C:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!uv pip install --upgrade pip\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!uv pip install sagemaker boto3 mlflow docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify PyTorch GPU Support\n",
    "\n",
    "Check if PyTorch can detect and use your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "Tensor on CPU:\n",
      "tensor([[0.8017, 0.7165, 0.1276],\n",
      "        [0.2206, 0.1958, 0.7876],\n",
      "        [0.1882, 0.0364, 0.7397],\n",
      "        [0.5407, 0.3389, 0.8835],\n",
      "        [0.4147, 0.3015, 0.7244]])\n",
      "\n",
      "Tensor on GPU:\n",
      "tensor([[0.8017, 0.7165, 0.1276],\n",
      "        [0.2206, 0.1958, 0.7876],\n",
      "        [0.1882, 0.0364, 0.7397],\n",
      "        [0.5407, 0.3389, 0.8835],\n",
      "        [0.4147, 0.3015, 0.7244]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Simple GPU test\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.rand(5, 3)\n",
    "    print(\"\\nTensor on CPU:\")\n",
    "    print(x)\n",
    "    print(\"\\nTensor on GPU:\")\n",
    "    print(x.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Docker\n",
    "\n",
    "Set up Docker with NVIDIA Container Toolkit support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker installed: Docker version 27.0.3, build 7d4bcd8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Docker installation\n",
    "try:\n",
    "    docker_version = subprocess.check_output([\"docker\", \"--version\"])\n",
    "    print(f\"Docker installed: {docker_version.decode()}\")\n",
    "except:\n",
    "    print(\"Docker not detected. Please install Docker Desktop for Windows\")\n",
    "    print(\"Visit: https://docs.docker.com/desktop/windows/install/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NVIDIA Container Toolkit\n",
    "\n",
    "Follow these steps manually:\n",
    "\n",
    "1. Install NVIDIA Container Toolkit:\n",
    "```bash\n",
    "distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y nvidia-docker2\n",
    "```\n",
    "\n",
    "2. Restart Docker service:\n",
    "```bash\n",
    "sudo systemctl restart docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure SageMaker Local Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker_core\\main\\shapes.py:5799: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  \"\"\"\n",
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker_core\\main\\shapes.py:6604: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"\n",
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker_core\\main\\shapes.py:8812: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"\n",
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\hohoy\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\smdebug_rulesconfig\\actions\\utils.py:5: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  TRAINING_JOB_PREFIX_REGEX = \"^[A-Za-z0-9\\-]+$\"\n",
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\smdebug_rulesconfig\\actions\\utils.py:6: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  EMAIL_ADDRESS_REGEX = \"^[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"\n",
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\smdebug_rulesconfig\\actions\\utils.py:7: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  PHONE_NUMBER_REGEX = \"^\\+\\d{1,15}$\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/16/25 19:11:16] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Windows Support for Local Mode is Experimental                    <a href=\"file://c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker\\local\\local_session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">local_session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker\\local\\local_session.py#698\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">698</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/16/25 19:11:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Windows Support for Local Mode is Experimental                    \u001b]8;id=953969;file://c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker\\local\\local_session.py\u001b\\\u001b[2mlocal_session.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=264172;file://c:\\Users\\hohoy\\OneDrive\\Desktop\\sagemaker-ncf-mlflow\\.venv\\Lib\\site-packages\\sagemaker\\local\\local_session.py#698\u001b\\\u001b[2m698\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.239.1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Hiding some warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Configure SageMaker session\n",
    "sagemaker_session = sagemaker.LocalSession()\n",
    "boto_session = boto3.Session(region_name='ap-southeast-2')\n",
    "\n",
    "# Set up role (not needed for local mode, but required for API compatibility)\n",
    "role = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\n",
    "\n",
    "print(\"SageMaker version:\", sagemaker.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up MLflow\n",
    "\n",
    "Configure MLflow for experiment tracking.\n",
    "\n",
    "Before running this cell, make sure to start the MLflow server in a separate terminal:\n",
    "\n",
    "```bash\n",
    "uv run mlflow server --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 2.20.2\n",
      "MLflow tracking URI: http://localhost:5000\n",
      "Experiment ID: 212974742724506544\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Disable warning logs that show file paths\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)\n",
    "logging.getLogger('botocore.credentials').setLevel(logging.ERROR)\n",
    "\n",
    "# Configure MLflow\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'http://localhost:5000'\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "# Create experiment with error handling\n",
    "experiment_name = \"ncf-local-development\"\n",
    "try:\n",
    "    # First check if experiment exists\n",
    "    existing_exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if existing_exp:\n",
    "        experiment_id = existing_exp.experiment_id\n",
    "    else:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        \n",
    "    print(f\"MLflow version: {mlflow.__version__}\")\n",
    "    print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Print the actual error for debugging\n",
    "    print(f\"Error encountered: {str(e)}\")\n",
    "    print(\"\\nUnable to connect to MLflow server. Possible causes:\")\n",
    "    print(\"- MLflow server not running. Run 'mlflow server --port 5000'\")\n",
    "    print(\"- Old MLflow version. Upgrade using 'pip install --upgrade mlflow'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify GPU Container Support\n",
    "\n",
    "Test NVIDIA Container Toolkit with a simple PyTorch container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1-cuda11.7-cudnn8-runtime: Pulling from pytorch/pytorch\n",
      "Digest: sha256:82e0d379a5dedd6303c89eda57bcc434c40be11f249ddfadfd5673b84351e806\n",
      "Status: Image is up to date for pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
      "docker.io/pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Pull PyTorch container\n",
    "!docker pull pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
    "\n",
    "# Run container with GPU support\n",
    "!docker run --gpus all pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime python -c \\\n",
    "    \"import torch; print('CUDA available:', torch.cuda.is_available())\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup Complete\n",
    "\n",
    "If all cells executed successfully, your local development environment is ready for NCF development with SageMaker local mode and GPU support.\n",
    "\n",
    "Next steps:\n",
    "1. Ensure MLflow server is running: `mlflow server --port 5000`\n",
    "2. Proceed to Lesson 2: Data Preparation for NCF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
